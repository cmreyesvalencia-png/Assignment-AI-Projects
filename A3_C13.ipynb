{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmreyesvalencia-png/Assignment-AI-Projects/blob/main/A3_C13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45c55a4f-943d-4983-b14f-ec38b886ef24",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "45c55a4f-943d-4983-b14f-ec38b886ef24"
      },
      "source": [
        "# **Project 3: Machine Learning for Predicting Trading Signals**\n",
        "- **Course:** Data Analytics and Business Intelligence Analyst\n",
        "- **Institution:** Willis College\n",
        "- **Student Name:** Carlos Reyes\n",
        "- **Instructor:** Ratinder Rajpal\n",
        "- **Date:** 2025 Nov, 22"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8124fbb-c4ea-437c-b728-6b8b8a9918c3",
      "metadata": {
        "id": "b8124fbb-c4ea-437c-b728-6b8b8a9918c3"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "- This assignment develop and evaluate supervised machine learning models to predict â€™buyâ€™ and â€™sellâ€™ signals for stocks, using manually computed technical indicators such as MACD, RSI, and Bollinger Bands. This project aims to apply advanced classification techniques to financial market data, facilitating effective trading decision-making.\n",
        "\n",
        "- The assignment implement various machine learning models to generate predictive trading signals and evaluate these models based on their accuracy, precision, recall, and overall predictive power. Insights into the behavior of financial markets through technical analysis will also be gained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6946a9b2-2cb4-4ab7-8c06-ad970867e53a",
      "metadata": {
        "id": "6946a9b2-2cb4-4ab7-8c06-ad970867e53a",
        "outputId": "b9b2d821-9e00-4934-a86a-5ae58e22a4fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Step 1 â€” Imports and setup\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import joblib\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "\n",
        "# Directories\n",
        "INPUT_PARQUET = \"clean_stock_data.parquet\"\n",
        "OUTPUT_DIR = \"outputs\"\n",
        "MODEL_DIR = \"models\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e0e4138-d535-45fa-a6ea-af88456c69a1",
      "metadata": {
        "id": "0e0e4138-d535-45fa-a6ea-af88456c69a1",
        "outputId": "101e3dd6-3268-48e3-e912-faaa37018247"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading cleaned data...\n",
            "Loaded data shape: (16557818, 66)\n"
          ]
        }
      ],
      "source": [
        "# Step 2 â€” Load Parquet\n",
        "print(\"Loading cleaned data...\")\n",
        "df = pd.read_parquet(INPUT_PARQUET)\n",
        "df = df.reset_index(drop=True)\n",
        "print(\"Loaded data shape:\", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80ad656f-9e29-41cc-afb7-f5a92f7af916",
      "metadata": {
        "id": "80ad656f-9e29-41cc-afb7-f5a92f7af916"
      },
      "source": [
        "# **1. Feature Engineering with Technical Indicators**\n",
        "\n",
        "In this task we are utilizing domain knowledge for feature engineering. Manually compute MACD and RSI for each stock (By \"manually,\" we refer to directly coding the formula using Python and Pandas, without relying on pre-built functions from external libraries or tools). Define buy, sell, and hold signals based on the behavior of these indicators. The signal is Buy if both indicators are showing Buy, and the signal is Sell if both indicators are showing Sell signal, otherwise the signal is to Hold.\n",
        "\n",
        "## Moving Average Convergence Divergence (MACD)\n",
        "\n",
        "**Definition:** MACD is calculated by subtracting the 26-period Exponential Moving Average (EMA) from the 12-period EMA. The result of this subtraction is the MACD line. A signal line, which is the 9-period EMA of the MACD line, is then plotted alongside the MACD line to act as a trigger for buy and sell signals.\n",
        "\n",
        "- **MACD** = EMAâ‚â‚‚(close) â€“ EMAâ‚‚â‚†(close)\n",
        "- **Signal Line** = EMAâ‚‰(MACD)\n",
        "\n",
        "**Signals:**\n",
        "- **Buy Signal:** When the MACD crosses above the signal line\n",
        "- **Sell Signal:** When the MACD crosses below the signal line\n",
        "\n",
        "## Relative Strength Index (RSI)\n",
        "\n",
        "**Definition:** RSI is a momentum oscillator that measures the speed and change of price movements. RSI oscillates between zero and 100. Traditionally, RSI is considered overbought when above 70 and oversold when below 30.\n",
        "\n",
        "- **RSI** = 100 - (100 Ã· (1 + RS))\n",
        "- Where **RS (Relative Strength)** is the average gain divided by the average loss:\n",
        "- **RS** = EMA(Gain) / EMA(Loss)\n",
        "- These averages are typically calculated using an exponential moving average\n",
        "\n",
        "**Signals:**\n",
        "- **Buy Signal:** RSI levels below 30 suggest a potential upward turn\n",
        "- **Sell Signal:** RSI levels above 70 suggest a potential downward turn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "350731c6-1194-4cf6-867b-3e47ad72c71e",
      "metadata": {
        "id": "350731c6-1194-4cf6-867b-3e47ad72c71e"
      },
      "outputs": [],
      "source": [
        "# Step 3 â€” Indicator calculation functions\n",
        "def compute_ema(series: pd.Series, span: int) -> pd.Series:\n",
        "    \"\"\"Compute exponential moving average.\"\"\"\n",
        "    return series.ewm(span=span, adjust=False).mean()\n",
        "\n",
        "def compute_macd(df: pd.DataFrame, price_col: str = \"close\") -> pd.DataFrame:\n",
        "    \"\"\"Compute MACD and MACD signal per ticker.\"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"MACD\"] = df.groupby(\"ticker\")[price_col].transform(lambda s: compute_ema(s, 12) - compute_ema(s, 26))\n",
        "    df[\"MACD_signal\"] = df.groupby(\"ticker\")[\"MACD\"].transform(lambda s: compute_ema(s, 9))\n",
        "    return df\n",
        "\n",
        "def compute_rsi(df: pd.DataFrame, price_col: str = \"close\", period: int = 14) -> pd.DataFrame:\n",
        "    \"\"\"Compute RSI per ticker.\"\"\"\n",
        "    df = df.copy()\n",
        "    def rsi_for_series(close: pd.Series) -> pd.Series:\n",
        "        delta = close.diff()\n",
        "        gain = delta.where(delta > 0, 0.0)\n",
        "        loss = -delta.where(delta < 0, 0.0)\n",
        "        avg_gain = gain.ewm(span=period, adjust=False).mean()\n",
        "        avg_loss = loss.ewm(span=period, adjust=False).mean()\n",
        "        rs = avg_gain / (avg_loss.replace(0, np.nan))\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "        return rsi.fillna(50)\n",
        "    df[\"RSI\"] = df.groupby(\"ticker\")[price_col].transform(rsi_for_series)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20af73d7-18aa-4825-8216-a35a8cee2fe1",
      "metadata": {
        "id": "20af73d7-18aa-4825-8216-a35a8cee2fe1"
      },
      "outputs": [],
      "source": [
        "# Step 4 â€” Generate signals\n",
        "def generate_indicator_signals(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Generate trading signals using vectorized operations.\"\"\"\n",
        "    df = df.copy().sort_values([\"ticker\", \"date\"])\n",
        "\n",
        "    print(\"Generating MACD signals...\")\n",
        "    df[\"MACD_prev\"] = df.groupby(\"ticker\")[\"MACD\"].shift(1)\n",
        "    df[\"MACD_signal_prev\"] = df.groupby(\"ticker\")[\"MACD_signal\"].shift(1)\n",
        "\n",
        "    conditions = [\n",
        "        (df[\"MACD_prev\"] < df[\"MACD_signal_prev\"]) & (df[\"MACD\"] > df[\"MACD_signal\"]),\n",
        "        (df[\"MACD_prev\"] > df[\"MACD_signal_prev\"]) & (df[\"MACD\"] < df[\"MACD_signal\"])\n",
        "    ]\n",
        "    choices = [\"Buy\", \"Sell\"]\n",
        "    df[\"MACD_flag\"] = np.select(conditions, choices, default=\"Neutral\")\n",
        "\n",
        "    print(\"Generating RSI signals...\")\n",
        "    conditions_rsi = [\n",
        "        df[\"RSI\"] < 30,\n",
        "        df[\"RSI\"] > 70\n",
        "    ]\n",
        "    choices_rsi = [\"Buy\", \"Sell\"]\n",
        "    df[\"RSI_flag\"] = np.select(conditions_rsi, choices_rsi, default=\"Neutral\")\n",
        "\n",
        "    print(\"Generating combined signals...\")\n",
        "    conditions_combined = [\n",
        "        (df[\"MACD_flag\"] == \"Buy\") & (df[\"RSI_flag\"] == \"Buy\"),\n",
        "        (df[\"MACD_flag\"] == \"Sell\") & (df[\"RSI_flag\"] == \"Sell\")\n",
        "    ]\n",
        "    choices_combined = [\"Buy\", \"Sell\"]\n",
        "    df[\"signal\"] = np.select(conditions_combined, choices_combined, default=\"Hold\")\n",
        "\n",
        "    df.drop(columns=[\"MACD_prev\", \"MACD_signal_prev\"], inplace=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a4ac806-6a7c-473b-b9d1-37c252762db7",
      "metadata": {
        "id": "7a4ac806-6a7c-473b-b9d1-37c252762db7",
        "outputId": "3ee680d8-0d0e-471e-a362-b1aad71f39b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing technical indicators...\n",
            "Generating trading signals...\n",
            "Generating MACD signals...\n",
            "Generating RSI signals...\n",
            "Generating combined signals...\n",
            "Signal distribution:\n",
            "signal\n",
            "Hold    16546033\n",
            "Sell        5937\n",
            "Buy         5848\n",
            "Name: count, dtype: int64\n",
            "Buy signals: 0.0353%\n",
            "Sell signals: 0.0359%\n"
          ]
        }
      ],
      "source": [
        "# Compute technical indicators\n",
        "print(\"Computing technical indicators...\")\n",
        "df = compute_macd(df)\n",
        "df = compute_rsi(df)\n",
        "\n",
        "# Generate signals\n",
        "print(\"Generating trading signals...\")\n",
        "df = generate_indicator_signals(df)\n",
        "\n",
        "print(\"Signal distribution:\")\n",
        "print(df['signal'].value_counts())\n",
        "print(f\"Buy signals: {(df['signal'] == 'Buy').sum() / len(df) * 100:.4f}%\")\n",
        "print(f\"Sell signals: {(df['signal'] == 'Sell').sum() / len(df) * 100:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71115853-601c-4c62-85f5-bd54c8be1ac6",
      "metadata": {
        "id": "71115853-601c-4c62-85f5-bd54c8be1ac6"
      },
      "source": [
        "# 2. Data Preparation and Splitting\n",
        "- Integrate the indicators and the signals into the main dataset. Use the signals computed above as ground-truth labels for the dataset. Split the data into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4b407ed-eab3-4505-942a-dcdca9a7270a",
      "metadata": {
        "id": "c4b407ed-eab3-4505-942a-dcdca9a7270a",
        "outputId": "903c12df-fc0e-40db-b93c-7ec5b1ddf6ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing ML dataset...\n",
            "Final dataset - X shape: (16557818, 13), y shape: (16557818,)\n",
            "Label distribution:\n",
            "label\n",
            "0    16546033\n",
            "1        5848\n",
            "2        5937\n",
            "Name: count, dtype: int64\n",
            "Sampling to 1M records...\n",
            "Splitting data...\n",
            "Train: (700000, 13), Val: (150000, 13), Test: (150000, 13)\n"
          ]
        }
      ],
      "source": [
        "# Step 5 â€” Prepare features and labels\n",
        "def prepare_ml_dataset(df: pd.DataFrame):\n",
        "    \"\"\"Prepare dataset for machine learning.\"\"\"\n",
        "    df = df.copy().sort_values([\"ticker\", \"date\"])\n",
        "\n",
        "    base_features = [\"open\",\"high\",\"low\",\"close\",\"volume\",\"ma_7\",\"ma_30\",\"volatility_30\",\"daily_return\",\"MACD\",\"MACD_signal\",\"RSI\"]\n",
        "    features = [c for c in base_features if c in df.columns]\n",
        "\n",
        "    df[\"ticker_id\"] = df[\"ticker\"].astype(\"category\").cat.codes\n",
        "    features.append(\"ticker_id\")\n",
        "\n",
        "    if \"signal\" not in df.columns:\n",
        "        raise KeyError(\"'signal' column not found.\")\n",
        "\n",
        "    df = df.dropna(subset=features + [\"signal\"]).copy()\n",
        "    df[\"label\"] = df[\"signal\"].map({\"Buy\":1, \"Hold\":0, \"Sell\":2})\n",
        "\n",
        "    X = df[features].copy()\n",
        "    y = df[\"label\"].copy()\n",
        "\n",
        "    print(f\"Final dataset - X shape: {X.shape}, y shape: {y.shape}\")\n",
        "    print(\"Label distribution:\")\n",
        "    print(y.value_counts().sort_index())\n",
        "\n",
        "    return X, y, df\n",
        "\n",
        "print(\"Preparing ML dataset...\")\n",
        "X, y, df_full = prepare_ml_dataset(df)\n",
        "\n",
        "# Sample for memory efficiency\n",
        "if len(X) > 1000000:\n",
        "    print(\"Sampling to 1M records...\")\n",
        "    sample_idx = X.sample(n=1000000, random_state=42).index\n",
        "    X = X.loc[sample_idx]\n",
        "    y = y.loc[sample_idx]\n",
        "    df_full = df_full.loc[sample_idx]\n",
        "\n",
        "# Split data\n",
        "print(\"Splitting data...\")\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, shuffle=False, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.17647, shuffle=False, random_state=42)\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7352b5d3-cb8a-4e1e-96c9-7c52cf80d538",
      "metadata": {
        "id": "7352b5d3-cb8a-4e1e-96c9-7c52cf80d538"
      },
      "source": [
        "# 3. Model Building and Validation\n",
        "- Implement 1-Logistic Regression, 2-Random Forests, 3-Support Vector Machines (SVM). Train the models on the training set and validate their performance. Training them can be time-consuming, depending on your computer's processing power."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71ff2e99-e1cd-4f37-a318-013375c40e95",
      "metadata": {
        "id": "71ff2e99-e1cd-4f37-a318-013375c40e95"
      },
      "outputs": [],
      "source": [
        "# Step 6 â€” ORIGINAL APPROACH (Baseline)\n",
        "def original_approach(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Original training approach with basic class weighting.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ORIGINAL APPROACH - Training Models\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    trained_models = {}\n",
        "    results = []\n",
        "\n",
        "    pipelines = {\n",
        "        \"LogisticRegression\": (Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"clf\", LogisticRegression(max_iter=200, class_weight=\"balanced\", random_state=42))\n",
        "        ]), {\n",
        "            \"clf__C\": [0.01, 0.1, 1.0]\n",
        "        }),\n",
        "        \"RandomForest\": (Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"clf\", RandomForestClassifier(class_weight=\"balanced\", n_jobs=-1, random_state=42))\n",
        "        ]), {\n",
        "            \"clf__n_estimators\": [50, 100],\n",
        "            \"clf__max_depth\": [5, 10, None]\n",
        "        }),\n",
        "        \"SVM_Linear\": (Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"clf\", SVC(kernel='linear', class_weight=\"balanced\", cache_size=1000, random_state=42))\n",
        "        ]), {\n",
        "            \"clf__C\": [0.1, 1.0]\n",
        "        })\n",
        "    }\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=3, shuffle=False)\n",
        "\n",
        "    for name, (pipe, params) in pipelines.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "\n",
        "        if name == \"SVM_Linear\" and len(X_train) > 50000:\n",
        "            print(\"Sampling 50,000 records for SVM training...\")\n",
        "            # CORRECTION: Use pandas sample instead of np.random.choice\n",
        "            sample_indices = X_train.sample(n=50000, random_state=42).index\n",
        "            X_train_model = X_train.loc[sample_indices]\n",
        "            y_train_model = y_train.loc[sample_indices]\n",
        "            svm_skf = StratifiedKFold(n_splits=2, shuffle=False)\n",
        "            grid = GridSearchCV(pipe, params, cv=svm_skf, scoring=\"f1_weighted\", n_jobs=-1)\n",
        "            grid.fit(X_train_model, y_train_model)\n",
        "        else:\n",
        "            grid = GridSearchCV(pipe, params, cv=skf, scoring=\"f1_weighted\", n_jobs=-1)\n",
        "            grid.fit(X_train, y_train)\n",
        "\n",
        "        best_model = grid.best_estimator_\n",
        "        trained_models[name] = best_model\n",
        "\n",
        "        y_pred = best_model.predict(X_test)\n",
        "\n",
        "        # Basic metrics\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
        "        rec = recall_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
        "        f1 = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
        "\n",
        "        # Minority class metrics\n",
        "        buy_recall = recall_score(y_test, y_pred, labels=[1], average=None)[0] if 1 in y_test.unique() else 0\n",
        "        sell_recall = recall_score(y_test, y_pred, labels=[2], average=None)[0] if 2 in y_test.unique() else 0\n",
        "        macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "        print(f\"{name} â€” Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
        "        print(f\"Buy Recall: {buy_recall:.4f}, Sell Recall: {sell_recall:.4f}, Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "        results.append({\n",
        "            \"model\": name,\n",
        "            \"best_params\": grid.best_params_,\n",
        "            \"accuracy\": acc,\n",
        "            \"precision_weighted\": prec,\n",
        "            \"recall_weighted\": rec,\n",
        "            \"f1_weighted\": f1,\n",
        "            \"macro_f1\": macro_f1,\n",
        "            \"buy_recall\": buy_recall,\n",
        "            \"sell_recall\": sell_recall\n",
        "        })\n",
        "\n",
        "    return trained_models, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb3fd82a-ebde-49dc-9684-0b334992d013",
      "metadata": {
        "id": "cb3fd82a-ebde-49dc-9684-0b334992d013"
      },
      "outputs": [],
      "source": [
        "# Step 7 â€” OPTIMIZED ENHANCED APPROACH\n",
        "def enhanced_approach_optimized(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"OPTIMIZED Enhanced approach with faster training.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"OPTIMIZED ENHANCED APPROACH - Fast Training\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    trained_models = {}\n",
        "    results = []\n",
        "\n",
        "    def evaluate_model_enhanced(model, X_test, y_test, name):\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Focus on key metrics only for speed\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        # Calculate minority class recall directly (faster)\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        if cm.shape[0] >= 2:  # Ensure we have at least 2 classes\n",
        "            buy_recall = cm[1,1] / cm[1,:].sum() if cm[1,:].sum() > 0 else 0\n",
        "            sell_recall = cm[2,2] / cm[2,:].sum() if cm.shape[0] >= 3 and cm[2,:].sum() > 0 else 0\n",
        "        else:\n",
        "            buy_recall, sell_recall = 0, 0\n",
        "\n",
        "        macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "        print(f\"\\n{name} Performance:\")\n",
        "        print(f\"Accuracy: {acc:.4f}, Macro F1: {macro_f1:.4f}\")\n",
        "        print(f\"Buy Recall: {buy_recall:.4f}, Sell Recall: {sell_recall:.4f}\")\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(cm)\n",
        "\n",
        "        return {\n",
        "            \"accuracy\": acc,\n",
        "            \"macro_f1\": macro_f1,\n",
        "            \"buy_recall\": buy_recall,\n",
        "            \"sell_recall\": sell_recall,\n",
        "            \"confusion_matrix\": cm\n",
        "        }\n",
        "\n",
        "    # OPTIMIZED SMOTE - Create smaller balanced dataset\n",
        "    print(\"Applying OPTIMIZED SMOTE resampling...\")\n",
        "    print(\"Original class distribution:\", y_train.value_counts().sort_index())\n",
        "\n",
        "    # MAJOR OPTIMIZATION: Create smaller balanced dataset\n",
        "    original_counts = y_train.value_counts()\n",
        "\n",
        "    # Instead of oversampling to 699k each, create a manageable dataset\n",
        "    # Target: 50k majority, 25k each minority (total ~100k samples vs 2.1M)\n",
        "    target_majority = 50000\n",
        "    target_minority = 25000\n",
        "\n",
        "    print(f\"Optimized sampling targets - Majority: {target_majority}, Minority: {target_minority}\")\n",
        "\n",
        "    # First, sample the majority class down\n",
        "    majority_indices = X_train[y_train == 0].sample(n=target_majority, random_state=42).index\n",
        "    minority_buy_indices = X_train[y_train == 1].index\n",
        "    minority_sell_indices = X_train[y_train == 2].index\n",
        "\n",
        "    # Combine sampled majority with all minority\n",
        "    sampled_indices = list(majority_indices) + list(minority_buy_indices) + list(minority_sell_indices)\n",
        "    X_train_sampled = X_train.loc[sampled_indices]\n",
        "    y_train_sampled = y_train.loc[sampled_indices]\n",
        "\n",
        "    # Now apply SMOTE only to boost minority classes\n",
        "    smote = SMOTE(\n",
        "        random_state=42,\n",
        "        sampling_strategy={1: target_minority, 2: target_minority},\n",
        "        k_neighbors=3  # Reduce neighbors for speed\n",
        "    )\n",
        "\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_sampled, y_train_sampled)\n",
        "    print(\"Resampled class distribution:\", pd.Series(y_train_resampled).value_counts().sort_index())\n",
        "    print(f\"Dataset size reduced from ~2.1M to {len(X_train_resampled):,} samples\")\n",
        "\n",
        "    # OPTIMIZED pipelines with faster hyperparameters\n",
        "    pipelines = {\n",
        "        \"LogisticRegression_Enhanced\": (ImbPipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"clf\", LogisticRegression(max_iter=200, class_weight=\"balanced\", random_state=42))\n",
        "        ]), {\n",
        "            \"clf__C\": [0.1, 1.0]  # Reduced from 4 to 2 options\n",
        "        }),\n",
        "\n",
        "        \"RandomForest_Enhanced\": (ImbPipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"clf\", RandomForestClassifier(class_weight=\"balanced\", n_jobs=-1, random_state=42))\n",
        "        ]), {\n",
        "            \"clf__n_estimators\": [100],  # Reduced from 2 to 1 option\n",
        "            \"clf__max_depth\": [10, 20],  # Reduced from 3 to 2 options\n",
        "            \"clf__min_samples_split\": [10],  # Reduced options\n",
        "            \"clf__min_samples_leaf\": [2]     # Reduced options\n",
        "        }),\n",
        "\n",
        "        \"SVM_Linear_Enhanced\": (ImbPipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"clf\", SVC(kernel='linear', class_weight=\"balanced\", cache_size=2000, random_state=42))\n",
        "        ]), {\n",
        "            \"clf__C\": [1.0]  # Single best parameter from previous runs\n",
        "        })\n",
        "    }\n",
        "\n",
        "    for name, (pipe, params) in pipelines.items():\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Training {name}...\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # OPTIMIZATION: Use fewer CV folds\n",
        "        cv_strategy = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)  # Reduced from 3 to 2\n",
        "\n",
        "        # OPTIMIZATION: Reduce verbosity and use faster scoring\n",
        "        grid = GridSearchCV(\n",
        "            pipe, params, cv=cv_strategy, scoring=\"f1_macro\",\n",
        "            n_jobs=-1, verbose=0  # Reduced verbosity\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        grid.fit(X_train_resampled, y_train_resampled)\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        best_model = grid.best_estimator_\n",
        "        trained_models[name] = best_model\n",
        "\n",
        "        print(f\"Training completed in {training_time:.1f} seconds\")\n",
        "        print(f\"Best parameters: {grid.best_params_}\")\n",
        "\n",
        "        # Enhanced evaluation\n",
        "        metrics = evaluate_model_enhanced(best_model, X_test, y_test, name)\n",
        "        metrics.update({\n",
        "            \"model\": name,\n",
        "            \"best_params\": grid.best_params_,\n",
        "            \"training_time_seconds\": training_time\n",
        "        })\n",
        "        results.append(metrics)\n",
        "\n",
        "    return trained_models, results, X_train_resampled, y_train_resampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a6022d-6f5e-4be2-a740-ee057b3c42f1",
      "metadata": {
        "scrolled": true,
        "id": "48a6022d-6f5e-4be2-a740-ee057b3c42f1",
        "outputId": "14cf2697-c2a0-4c30-c71c-1ca99cb3e7df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting model training...\n",
            "\n",
            "================================================================================\n",
            "RUNNING ORIGINAL APPROACH\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "ORIGINAL APPROACH - Training Models\n",
            "============================================================\n",
            "\n",
            "Training LogisticRegression...\n",
            "LogisticRegression â€” Accuracy: 0.8768, Precision: 0.9994, Recall: 0.8768, F1: 0.9338\n",
            "Buy Recall: 1.0000, Sell Recall: 1.0000, Macro F1: 0.3182\n",
            "\n",
            "Training RandomForest...\n",
            "RandomForest â€” Accuracy: 0.9994, Precision: 0.9988, Recall: 0.9994, F1: 0.9991\n",
            "Buy Recall: 0.0000, Sell Recall: 0.0000, Macro F1: 0.3332\n",
            "\n",
            "Training SVM_Linear...\n",
            "Sampling 50,000 records for SVM training...\n",
            "SVM_Linear â€” Accuracy: 0.8542, Precision: 0.9994, Recall: 0.8542, F1: 0.9207\n",
            "Buy Recall: 1.0000, Sell Recall: 0.9750, Macro F1: 0.3129\n"
          ]
        }
      ],
      "source": [
        "# Step 8 â€” Run Both Approaches (CORRECTED)\n",
        "print(\"Starting model training...\")\n",
        "\n",
        "# Original Approach\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RUNNING ORIGINAL APPROACH\")\n",
        "print(\"=\"*80)\n",
        "original_models, original_results = original_approach(X_train, X_test, y_train, y_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bc3d494-88a0-48b5-8adc-d27a03fa1dc1",
      "metadata": {
        "id": "6bc3d494-88a0-48b5-8adc-d27a03fa1dc1",
        "outputId": "811d495c-111e-4eae-dab8-f11dc0a82901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "RUNNING OPTIMIZED ENHANCED APPROACH\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "OPTIMIZED ENHANCED APPROACH - Fast Training\n",
            "============================================================\n",
            "Applying OPTIMIZED SMOTE resampling...\n",
            "Original class distribution: label\n",
            "0    699508\n",
            "1       226\n",
            "2       266\n",
            "Name: count, dtype: int64\n",
            "Optimized sampling targets - Majority: 50000, Minority: 25000\n",
            "Resampled class distribution: label\n",
            "0    50000\n",
            "1    25000\n",
            "2    25000\n",
            "Name: count, dtype: int64\n",
            "Dataset size reduced from ~2.1M to 100,000 samples\n",
            "\n",
            "==================================================\n",
            "Training LogisticRegression_Enhanced...\n",
            "==================================================\n",
            "Training completed in 7.6 seconds\n",
            "Best parameters: {'clf__C': 1.0}\n",
            "\n",
            "LogisticRegression_Enhanced Performance:\n",
            "Accuracy: 0.8820, Macro F1: 0.3195\n",
            "Buy Recall: 1.0000, Sell Recall: 1.0000\n",
            "Confusion Matrix:\n",
            "[[132213   7942   9752]\n",
            " [     0     53      0]\n",
            " [     0      0     40]]\n",
            "\n",
            "==================================================\n",
            "Training RandomForest_Enhanced...\n",
            "==================================================\n",
            "Training completed in 28.3 seconds\n",
            "Best parameters: {'clf__max_depth': 20, 'clf__min_samples_leaf': 2, 'clf__min_samples_split': 10, 'clf__n_estimators': 100}\n",
            "\n",
            "RandomForest_Enhanced Performance:\n",
            "Accuracy: 0.9701, Macro F1: 0.3503\n",
            "Buy Recall: 0.8113, Sell Recall: 0.7500\n",
            "Confusion Matrix:\n",
            "[[145444   1874   2589]\n",
            " [    10     43      0]\n",
            " [    10      0     30]]\n",
            "\n",
            "==================================================\n",
            "Training SVM_Linear_Enhanced...\n",
            "==================================================\n",
            "Training completed in 251.6 seconds\n",
            "Best parameters: {'clf__C': 1.0}\n",
            "\n",
            "SVM_Linear_Enhanced Performance:\n",
            "Accuracy: 0.8791, Macro F1: 0.3188\n",
            "Buy Recall: 1.0000, Sell Recall: 1.0000\n",
            "Confusion Matrix:\n",
            "[[131777   8158   9972]\n",
            " [     0     53      0]\n",
            " [     0      0     40]]\n"
          ]
        }
      ],
      "source": [
        "# Enhanced Approach - NOW USING OPTIMIZED VERSION\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RUNNING OPTIMIZED ENHANCED APPROACH\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "enhanced_models, enhanced_results, X_train_resampled, y_train_resampled = enhanced_approach_optimized(X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "474c63e9-8d14-472a-b514-5df410b29bd4",
      "metadata": {
        "id": "474c63e9-8d14-472a-b514-5df410b29bd4"
      },
      "source": [
        "# **4. Model Evaluation and Optimization**\n",
        "- Evaluate the models on the test set. Optimize the models based on evaluation metrics and adjust hyperparameters as needed to improve performance. Use cross-validation where applicable to ensure the robustness of the evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09e8aa34-ef65-45fe-9d3f-dae7d04711c1",
      "metadata": {
        "id": "09e8aa34-ef65-45fe-9d3f-dae7d04711c1",
        "outputId": "d91e1cf7-c8be-48ed-acae-f6e4ece2493f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "PERFORMANCE COMPARISON: BASELINE vs OPTIMIZED ENHANCED\n",
            "================================================================================\n",
            "ðŸ“Š DIRECT PERFORMANCE COMPARISON\n",
            "------------------------------------------------------------\n",
            "\n",
            "LogisticRegression:\n",
            "  Accuracy:    0.8768 â†’ 0.8820 (+0.52%)\n",
            "  Buy Recall:  1.0000 â†’ 1.0000 (+0.00%)\n",
            "  Sell Recall: 1.0000 â†’ 1.0000 (+0.00%)\n",
            "  Macro F1:    0.3182 â†’ 0.3195 (+0.13%)\n",
            "\n",
            "RandomForest:\n",
            "  Accuracy:    0.9994 â†’ 0.9701 (-2.93%)\n",
            "  Buy Recall:  0.0000 â†’ 0.8113 (+81.13%)\n",
            "  Sell Recall: 0.0000 â†’ 0.7500 (+75.00%)\n",
            "  Macro F1:    0.3332 â†’ 0.3503 (+1.71%)\n",
            "\n",
            "SVM Linear:\n",
            "  Accuracy:    0.8542 â†’ 0.8791 (+2.50%)\n",
            "  Buy Recall:  1.0000 â†’ 1.0000 (+0.00%)\n",
            "  Sell Recall: 0.9750 â†’ 1.0000 (+2.50%)\n",
            "  Macro F1:    0.3129 â†’ 0.3188 (+0.59%)\n",
            "\n",
            "============================================================\n",
            "SUMMARY STATISTICS\n",
            "============================================================\n",
            "Average Accuracy Change:    +0.03%\n",
            "Average Buy Recall Change:  +27.04%\n",
            "Average Sell Recall Change: +25.83%\n",
            "Average Macro F1 Change:    +0.81%\n"
          ]
        }
      ],
      "source": [
        "# Step 9 â€” BASELINE vs OPTIMIZED PERFORMANCE COMPARISON\n",
        "def compare_approaches(baseline_results, optimized_results):\n",
        "    \"\"\"Compare Baseline vs Optimized Enhanced approach performance\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PERFORMANCE COMPARISON: BASELINE vs OPTIMIZED ENHANCED\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    comparison_data = []\n",
        "\n",
        "    print(\"ðŸ“Š DIRECT PERFORMANCE COMPARISON\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Compare each model type\n",
        "    for base_model in ['LogisticRegression', 'RandomForest', 'SVM_Linear']:\n",
        "        # Find baseline results\n",
        "        baseline_model = next((r for r in baseline_results if base_model in r['model']), None)\n",
        "        # Find optimized results\n",
        "        optimized_model = next((r for r in optimized_results if base_model in r['model']), None)\n",
        "\n",
        "        if baseline_model and optimized_model:\n",
        "            # Calculate changes\n",
        "            accuracy_change = optimized_model['accuracy'] - baseline_model['accuracy']\n",
        "            buy_recall_change = optimized_model['buy_recall'] - baseline_model['buy_recall']\n",
        "            sell_recall_change = optimized_model['sell_recall'] - baseline_model['sell_recall']\n",
        "            macro_f1_change = optimized_model['macro_f1'] - baseline_model['macro_f1']\n",
        "\n",
        "            # Display comparison\n",
        "            model_name = base_model.replace('_', ' ')\n",
        "            print(f\"\\n{model_name}:\")\n",
        "            print(f\"  Accuracy:    {baseline_model['accuracy']:.4f} â†’ {optimized_model['accuracy']:.4f} ({accuracy_change*100:+.2f}%)\")\n",
        "            print(f\"  Buy Recall:  {baseline_model['buy_recall']:.4f} â†’ {optimized_model['buy_recall']:.4f} ({buy_recall_change*100:+.2f}%)\")\n",
        "            print(f\"  Sell Recall: {baseline_model['sell_recall']:.4f} â†’ {optimized_model['sell_recall']:.4f} ({sell_recall_change*100:+.2f}%)\")\n",
        "            print(f\"  Macro F1:    {baseline_model['macro_f1']:.4f} â†’ {optimized_model['macro_f1']:.4f} ({macro_f1_change*100:+.2f}%)\")\n",
        "\n",
        "            # Store comparison\n",
        "            comparison_data.append({\n",
        "                'Model': model_name,\n",
        "                'Accuracy_Baseline': baseline_model['accuracy'],\n",
        "                'Accuracy_Optimized': optimized_model['accuracy'],\n",
        "                'Buy_Recall_Baseline': baseline_model['buy_recall'],\n",
        "                'Buy_Recall_Optimized': optimized_model['buy_recall'],\n",
        "                'Sell_Recall_Baseline': baseline_model['sell_recall'],\n",
        "                'Sell_Recall_Optimized': optimized_model['sell_recall'],\n",
        "                'Macro_F1_Baseline': baseline_model['macro_f1'],\n",
        "                'Macro_F1_Optimized': optimized_model['macro_f1']\n",
        "            })\n",
        "\n",
        "    # Create comparison DataFrame\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "    # Summary statistics\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SUMMARY STATISTICS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if not comparison_df.empty:\n",
        "        accuracy_improvement = (comparison_df['Accuracy_Optimized'] - comparison_df['Accuracy_Baseline']).mean()\n",
        "        buy_recall_improvement = (comparison_df['Buy_Recall_Optimized'] - comparison_df['Buy_Recall_Baseline']).mean()\n",
        "        sell_recall_improvement = (comparison_df['Sell_Recall_Optimized'] - comparison_df['Sell_Recall_Baseline']).mean()\n",
        "        macro_f1_improvement = (comparison_df['Macro_F1_Optimized'] - comparison_df['Macro_F1_Baseline']).mean()\n",
        "\n",
        "        print(f\"Average Accuracy Change:    {accuracy_improvement*100:+.2f}%\")\n",
        "        print(f\"Average Buy Recall Change:  {buy_recall_improvement*100:+.2f}%\")\n",
        "        print(f\"Average Sell Recall Change: {sell_recall_improvement*100:+.2f}%\")\n",
        "        print(f\"Average Macro F1 Change:    {macro_f1_improvement*100:+.2f}%\")\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "# Run comparison\n",
        "comparison_df = compare_approaches(original_results, enhanced_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e92edb69-9793-413a-9d43-825fd8ba563e",
      "metadata": {
        "id": "e92edb69-9793-413a-9d43-825fd8ba563e",
        "outputId": "4b297948-30d4-4961-ee55-3d49124531d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving models and predictions...\n",
            "Saved LogisticRegression_Enhanced model to models\\logisticregression_enhanced_model.joblib\n",
            "Saved RandomForest_Enhanced model to models\\randomforest_enhanced_model.joblib\n",
            "Saved SVM_Linear_Enhanced model to models\\svm_linear_enhanced_model.joblib\n",
            "Saved LogisticRegression_Enhanced predictions to outputs\\predictions_LogisticRegression_Enhanced.parquet\n",
            "Saved RandomForest_Enhanced predictions to outputs\\predictions_RandomForest_Enhanced.parquet\n",
            "Saved SVM_Linear_Enhanced predictions to outputs\\predictions_SVM_Linear_Enhanced.parquet\n",
            "All models, predictions and results saved successfully!\n",
            "Models saved in: models\n",
            "Predictions and results saved in: outputs\n"
          ]
        }
      ],
      "source": [
        "# Step 10 â€” Save models and predictions\n",
        "print(\"Saving models and predictions...\")\n",
        "\n",
        "# Save enhanced models to MODEL_DIR\n",
        "for model_name, model in enhanced_models.items():\n",
        "    model_path = os.path.join(MODEL_DIR, f\"{model_name.lower()}_model.joblib\")\n",
        "    joblib.dump(model, model_path)\n",
        "    print(f\"Saved {model_name} model to {model_path}\")\n",
        "\n",
        "# Save predictions to OUTPUT_DIR\n",
        "for model_name, model in enhanced_models.items():\n",
        "    # Create predictions DataFrame\n",
        "    df_pred = df_full.loc[X_test.index, [\"date\", \"ticker\", \"close\", \"signal\"]].copy()\n",
        "    df_pred[f\"pred_{model_name}\"] = model.predict(X_test)\n",
        "\n",
        "    # Save predictions as Parquet\n",
        "    pred_path = os.path.join(OUTPUT_DIR, f\"predictions_{model_name}.parquet\")\n",
        "    df_pred.to_parquet(pred_path, index=False)\n",
        "    print(f\"Saved {model_name} predictions to {pred_path}\")\n",
        "\n",
        "# Save results to OUTPUT_DIR (remove problematic columns first)\n",
        "def clean_results(results):\n",
        "    \"\"\"Remove columns that can't be saved to Parquet\"\"\"\n",
        "    cleaned = []\n",
        "    for result in results:\n",
        "        clean_result = result.copy()\n",
        "        # Remove problematic columns\n",
        "        if 'confusion_matrix' in clean_result:\n",
        "            del clean_result['confusion_matrix']\n",
        "        if 'best_params' in clean_result:\n",
        "            # Convert dict to string if needed\n",
        "            if isinstance(clean_result['best_params'], dict):\n",
        "                clean_result['best_params'] = str(clean_result['best_params'])\n",
        "        cleaned.append(clean_result)\n",
        "    return cleaned\n",
        "\n",
        "# Clean and save results\n",
        "baseline_clean = clean_results(original_results)\n",
        "baseline_df = pd.DataFrame(baseline_clean)\n",
        "baseline_df.to_parquet(os.path.join(OUTPUT_DIR, \"baseline_results.parquet\"), index=False)\n",
        "\n",
        "enhanced_clean = clean_results(enhanced_results)\n",
        "enhanced_df = pd.DataFrame(enhanced_clean)\n",
        "enhanced_df.to_parquet(os.path.join(OUTPUT_DIR, \"enhanced_results.parquet\"), index=False)\n",
        "\n",
        "comparison_df.to_parquet(os.path.join(OUTPUT_DIR, \"comparison_results.parquet\"), index=False)\n",
        "\n",
        "print(\"All models, predictions and results saved successfully!\")\n",
        "print(f\"Models saved in: {MODEL_DIR}\")\n",
        "print(f\"Predictions and results saved in: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d34296ae-c252-42fd-a420-ed10d8c297b5",
      "metadata": {
        "id": "d34296ae-c252-42fd-a420-ed10d8c297b5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}